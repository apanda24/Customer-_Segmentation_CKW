{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data from the original CKW dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting unique ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "dataframes = {}\n",
    "dataframes['ckw01']=pl.read_csv(r'DATA/ckw_opendata_smartmeter_dataset_a_202101.csv.gz')\n",
    "\n",
    "ids = dataframes['ckw01']['id'].unique()\n",
    "print(f\"Number of Smart Meter IDs are {len(ids)}\")\n",
    "ids_df = pl.DataFrame({\"id\": ids})\n",
    "\n",
    "# Save the DataFrame to a CSV file change the location as required\n",
    "ids_df.write_csv(\"unique_ids.csv\")\n",
    "#unique_ids.csv now contains the unique smart meter ids in the CKW file used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving individual files per smart meter id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['ckw02']=pl.read_csv(r'DATA/ckw_opendata_smartmeter_dataset_a_202102.csv.gz') #monthly CKW file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the result DataFrames\n",
    "combined_house_dataframes = {}\n",
    "i = 0\n",
    "\n",
    "save_dir = r'DATA/try' #write the location where the files will be saved\n",
    "# Ensure the save directory exists\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Debug: Print all keys in dataframes\n",
    "print(\"Available months in dataframes:\", list(dataframes.keys()))\n",
    "\n",
    "# Process each common ID\n",
    "for house_id in ids[i:5]:  # Use .values to get the actual array of IDs\n",
    "   \n",
    "    # Initialize an empty list to store dataframes for the current house ID\n",
    "    house_data_list = []\n",
    "    print(f'Processing house ID: {house_id}, {i}')\n",
    "    \n",
    "    # Iterate over specific monthly dataframes\n",
    "    for key, df in list(dataframes.items()):  # Ensure the slice includes existing keys\n",
    "       \n",
    "        #house_data = df[df['id'] == house_id].copy()\n",
    "        house_data = df.filter(pl.col('id')==house_id)\n",
    "\n",
    "        # Check for duplicate rows within each day's data\n",
    "        house_data = house_data.unique(subset=['timestamp', 'id'])\n",
    "        #dropping id column to reduce size\n",
    "        house_data = house_data.drop('id')\n",
    "        \n",
    "        # Append the filtered dataframe to the list\n",
    "        house_data_list.append(house_data)\n",
    "    \n",
    "    if house_data_list:\n",
    "        # Concatenate all dataframes in the list\n",
    "        concatenated_house_data = pl.concat(house_data_list)\n",
    "    \n",
    "        # Store the concatenated dataframe in the dictionary\n",
    "        combined_house_dataframes[house_id] = concatenated_house_data\n",
    "    \n",
    "        file_name = f'{i}_data.csv' #change name as required\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "        df1 = concatenated_house_data.sort(by='timestamp')\n",
    "        df1.write_csv(file_path)\n",
    "        #print(f\"Data for house ID: {house_id} saved to {file_name}\")\n",
    "\n",
    "        # Verify the number of rows for each house ID\n",
    "        #print(f\"House ID {house_id} has {len(df1)} rows. Expected 8832 rows.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"No data found for house ID: {house_id} in the specified dataframes.\")\n",
    "        \n",
    "    i += 1\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting data for remaining ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes1={}\n",
    "dataframes1['ckw']=pd.read_csv(r'C:\\Users\\pana\\Desktop\\DATA\\ckw\\ckw_opendata_smartmeter_dataset_a_202107.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckw2=pd.read_csv(r'C:\\Users\\pana\\Desktop\\DATA\\ckw\\ckw_opendata_smartmeter_dataset_a_202102.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month ongoing: 6\n",
      "month ongoing: 7\n",
      "month ongoing: 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmonth_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfile_extension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Read the file and store it in the dictionary\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     dataframes[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mckw\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Access the DataFrames using the keys in the dictionary\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, df \u001b[38;5;129;01min\u001b[39;00m dataframes\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\parsers.pyx:2021\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\nilmtk-env\\lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\nilmtk-env\\lib\\gzip.py:487\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[0;32m    485\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[1;32m--> 487\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mprepend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the base file path and pattern\n",
    "base_path = r'C:\\DATA\\ckw\\raw\\ckw_opendata_smartmeter_dataset_a_2021'\n",
    "file_extension = '.csv.gz'\n",
    "compression_type = 'gzip'\n",
    "\n",
    "# Create a dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop to read files from 2 to 12 (corresponding to months)\n",
    "for month in range(6, 9):\n",
    "    # Format the month to ensure two digits (e.g., 03, 04, ..., 12)\n",
    "    print(f\"month ongoing: {month}\")\n",
    "    month_str = f'{month:02d}'\n",
    "    # Construct the full file path\n",
    "    file_path = f'{base_path}{month_str}{file_extension}'\n",
    "    # Read the file and store it in the dictionary\n",
    "    dataframes[f'ckw{month}'] = pl.read_csv(file_path, compression=compression_type)\n",
    "\n",
    "# Access the DataFrames using the keys in the dictionary\n",
    "for key, df in dataframes.items():\n",
    "    print(f'{key} DataFrame shape: {df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Debug: Print all keys in dataframes\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable months in dataframes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(\u001b[43mdataframes\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Process each common ID\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m house_id \u001b[38;5;129;01min\u001b[39;00m remaining_ids\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m49769\u001b[39m:]:  \u001b[38;5;66;03m# Use .values to get the actual array of IDs\u001b[39;00m\n\u001b[0;32m     18\u001b[0m    \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Initialize an empty list to store dataframes for the current house ID\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataframes' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary to store the result DataFrames\n",
    "combined_house_dataframes = {}\n",
    "i = 24182\n",
    "\n",
    "save_dir = r'C:\\Users\\pana\\Desktop\\DATA\\ckw\\2021\\summer_more'\n",
    "# Ensure the save directory exists\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Debug: Print all keys in dataframes\n",
    "print(\"Available months in dataframes:\", list(dataframes.keys()))\n",
    "\n",
    "# Process each common ID\n",
    "for house_id in remaining_ids.values[i:]:  # Use .values to get the actual array of IDs\n",
    "   \n",
    "    # Initialize an empty list to store dataframes for the current house ID\n",
    "    house_data_list = []\n",
    "    print(f'Processing house ID: {house_id}, {i}')\n",
    "    \n",
    "    # Iterate over specific monthly dataframes\n",
    "    for key, df in list(dataframes.items()):  # Ensure the slice includes existing keys\n",
    "        print(f'Processing dataframe: {key}')  # Debug: Print which dataframe is being processed\n",
    "        # Filter the current dataframe for the current house ID\n",
    "        house_data = df[df['id'] == house_id].copy()\n",
    "\n",
    "        # Check for duplicate rows within each day's data\n",
    "        house_data = house_data.drop_duplicates(subset=['timestamp', 'id'])\n",
    "        #dropping id column to reduce size\n",
    "        house_data = house_data.drop(columns =['id'])\n",
    "        \n",
    "        # Append the filtered dataframe to the list\n",
    "        house_data_list.append(house_data)\n",
    "    \n",
    "    if house_data_list:\n",
    "        # Concatenate all dataframes in the list\n",
    "        concatenated_house_data = pd.concat(house_data_list, ignore_index=True)\n",
    "    \n",
    "        # Store the concatenated dataframe in the dictionary\n",
    "        combined_house_dataframes[house_id] = concatenated_house_data\n",
    "    \n",
    "        file_name = f'{i}_summer_data.csv'\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "        df1 = concatenated_house_data.sort_values(by='timestamp')\n",
    "        df1.to_csv(file_path, index=False)\n",
    "        print(f\"Data for house ID: {house_id} saved to {file_name}\")\n",
    "\n",
    "        # Verify the number of rows for each house ID\n",
    "        print(f\"House ID {house_id} has {len(df1)} rows. Expected 8832 rows.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"No data found for house ID: {house_id} in the specified dataframes.\")\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "# Print results (optional)\n",
    "# for house_id, df in combined_house_dataframes.items():\n",
    "#     print(f\"\\nData for house ID: {house_id}\")\n",
    "#     print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming combined_house_dataframes already contains the data for the first 4 months for each house_id\n",
    "save_dir= r'C:\\Users\\pana\\Desktop\\DATA\\ckw\\2021\\spring'\n",
    "\n",
    "# Process each common ID\n",
    "for house_id in combined_house_dataframes:\n",
    "    # Get the existing concatenated dataframe for the current house_id\n",
    "    concatenated_house_data = combined_house_dataframes[house_id]\n",
    "    \n",
    "    # Initialize an empty list to store dataframes for the next 4 months\n",
    "    next_house_data_list = []\n",
    "    print(f'Adding next 4 months for house ID: {house_id}')\n",
    "    i=0\n",
    "    # Iterate over the next 4 monthly dataframes\n",
    "    for key, df in list(dataframes.items())[3:5]:  # Adjust the slice [4:8] for the next 4 months\n",
    "        print(f'Processing dataframe: {key}')\n",
    "        # Filter the current dataframe for the current house ID\n",
    "        house_data = df[df['id'] == house_id].copy()\n",
    "\n",
    "        # Check for duplicate rows within each day's data\n",
    "        house_data = house_data.drop_duplicates(subset=['timestamp', 'id'])\n",
    "        \n",
    "        # Append the filtered dataframe to the list\n",
    "        next_house_data_list.append(house_data)\n",
    "    \n",
    "    # Concatenate all dataframes for the next 4 months\n",
    "    concatenated_next_data = pd.concat(next_house_data_list, ignore_index=True)\n",
    "    \n",
    "    # Concatenate with the existing data for the house_id\n",
    "    updated_house_data = pd.concat([concatenated_house_data, concatenated_next_data], ignore_index=True)\n",
    "    \n",
    "    # Update the dictionary entry for the house_id\n",
    "    combined_house_dataframes[house_id] = updated_house_data\n",
    "\n",
    "# Now combined_house_dataframes contains the data for the spring months for each house_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming combined_house_dataframes already contains the data for the first 4 months for each house_id\n",
    "j=0\n",
    "# Process each common ID\n",
    "for house_id in combined_house_dataframes:\n",
    "    # Get the existing concatenated dataframe for the current house_id\n",
    "    concatenated_house_data = combined_house_dataframes[house_id]\n",
    "    \n",
    "    # Initialize an empty list to store dataframes for the next 4 months\n",
    "    next_house_data_list = []\n",
    "    print(f'Adding next 4 months for house ID: {house_id}, {j}')\n",
    "    \n",
    "    # Iterate over the next 4 monthly dataframes\n",
    "    for key, df in list(dataframes.items())[3:5]:  # Adjust the slice [4:8] for the next 4 months\n",
    "        print(f'Processing dataframe: {key}')\n",
    "        # Filter the current dataframe for the current house ID\n",
    "        house_data = df[df['id'] == house_id].copy()\n",
    "\n",
    "        # Check for duplicate rows within each day's data\n",
    "        house_data = house_data.drop_duplicates(subset=['timestamp', 'id'])\n",
    "        \n",
    "        # Append the filtered dataframe to the list\n",
    "        next_house_data_list.append(house_data)\n",
    "    \n",
    "    # Concatenate all dataframes for the next 4 months\n",
    "    concatenated_next_data = pd.concat(next_house_data_list, ignore_index=True)\n",
    "    \n",
    "    # Concatenate with the existing data for the house_id\n",
    "    updated_house_data = pd.concat([concatenated_house_data, concatenated_next_data], ignore_index=True)\n",
    "    \n",
    "    # Update the dictionary entry for the house_id\n",
    "    combined_house_dataframes[house_id] = updated_house_data\n",
    "    j=j+1\n",
    "\n",
    "# Now combined_house_dataframes contains the data for the first 8 months for each house_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each concatenated DataFrame to a CSV file\n",
    "i=4000\n",
    "for house_id, df in combined_house_dataframes.items():\n",
    "    file_name = f'{i}_data.csv'\n",
    "    df1=df.sort_values(by='timestamp')\n",
    "    df1.to_csv(file_name, index=False)\n",
    "    print(f\"Data for house ID: {house_id} saved to {file_name}\")\n",
    "\n",
    "    # Verify the number of rows for each house ID\n",
    "    print(f\"House ID {house_id} has {len(df1)} rows. Expected 35040 rows.\")\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for winter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "winter_timestamp_range = pl.datetime_range(datetime(2021,12,1), datetime(2022,2,28,23,45), \"15m\",eager=True)\n",
    "# Function to ensure the dataset has the full range of timestamps\n",
    "def fill_missing_timestamps(df, full_range):\n",
    "    # Convert the timestamp column to datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    \n",
    "    # Create a DataFrame with the full range of timestamps\n",
    "    full_range_df = pd.DataFrame({'timestamp': full_range})\n",
    "    full_range_df = pd.to_datetime(full_range_df['timestamp'], utc=True)\n",
    "    # Merge the original DataFrame with the full range DataFrame\n",
    "    merged_df = pd.merge(full_range_df, df, on='timestamp', how='left')\n",
    "    \n",
    "    # Fill missing values with 0\n",
    "    merged_df['value_kwh'].fillna(0, inplace=True)\n",
    "    \n",
    "    return merged_df\n",
    "if season == \"winter\":\n",
    "    for i,file_path in enumerate(file_paths):\n",
    "        df = pd.read_csv(file_path)\n",
    "        #df = df.tail( df.shape[0] -3)\n",
    "        #df.write_csv(file_path)\n",
    "        #print(f\"no. of rows in {i} =\",len(df))\n",
    "        if len(df)<8640:\n",
    "            # Ensure the dataset has the full range of timestamps and fill missing values\n",
    "            df_filled = fill_missing_timestamps(df, winter_timestamp_range)\n",
    "            # Save the updated dataset back to CSV\n",
    "            df_filled.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilmtk-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
