{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data from the original CKW dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting unique ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "dataframes = {}\n",
    "dataframes['ckw01']=pl.read_csv(r'DATA/ckw_opendata_smartmeter_dataset_a_202101.csv.gz')\n",
    "\n",
    "ids = dataframes['ckw01']['id'].unique()\n",
    "print(f\"Number of Smart Meter IDs are {len(ids)}\")\n",
    "ids_df = pl.DataFrame({\"id\": ids})\n",
    "\n",
    "# Save the DataFrame to a CSV file change the location as required\n",
    "ids_df.write_csv(\"unique_ids.csv\")\n",
    "#unique_ids.csv now contains the unique smart meter ids in the CKW file used.\n",
    "#save the unique ids from one file only. from then on use the csv file to extract other months of the same ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving individual files per smart meter id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['ckw02']=pl.read_csv(r'DATA/ckw_opendata_smartmeter_dataset_a_202102.csv.gz') #monthly CKW file path\n",
    "#load the monthly files individually to avoid crashing the system due to memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pl.read_csv(\"unique_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the result DataFrames\n",
    "combined_house_dataframes = {}\n",
    "i = 0\n",
    "\n",
    "save_dir = r'DATA/try' #write the location where the files will be saved\n",
    "# Ensure the save directory exists\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Debug: Print all keys in dataframes\n",
    "print(\"Available months in dataframes:\", list(dataframes.keys()))\n",
    "\n",
    "# Process each common ID\n",
    "for house_id in ids[i:]:  # Use .values to get the actual array of IDs\n",
    "   \n",
    "    # Initialize an empty list to store dataframes for the current house ID\n",
    "    house_data_list = []\n",
    "    print(f'Processing house ID: {house_id}, {i}')\n",
    "    \n",
    "    # Iterate over specific monthly dataframes\n",
    "    for key, df in list(dataframes.items()):  # Ensure the slice includes existing keys\n",
    "       \n",
    "        #house_data = df[df['id'] == house_id].copy()\n",
    "        house_data = df.filter(pl.col('id')==house_id)\n",
    "\n",
    "        # Check for duplicate rows within each day's data\n",
    "        house_data = house_data.unique(subset=['timestamp', 'id'])\n",
    "        #dropping id column to reduce size\n",
    "        house_data = house_data.drop('id')\n",
    "        \n",
    "        # Append the filtered dataframe to the list\n",
    "        house_data_list.append(house_data)\n",
    "    \n",
    "    if house_data_list:\n",
    "        # Concatenate all dataframes in the list\n",
    "        concatenated_house_data = pl.concat(house_data_list)\n",
    "    \n",
    "        # Store the concatenated dataframe in the dictionary\n",
    "        combined_house_dataframes[house_id] = concatenated_house_data\n",
    "    \n",
    "        file_name = f'{i}_data.csv' #change name as required\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "        df1 = concatenated_house_data.sort(by='timestamp')\n",
    "        df1.write_csv(file_path)\n",
    "        #print(f\"Data for house ID: {house_id} saved to {file_name}\")\n",
    "\n",
    "        # Verify the number of rows for each house ID\n",
    "        #print(f\"House ID {house_id} has {len(df1)} rows. Expected 8832 rows.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"No data found for house ID: {house_id} in the specified dataframes.\")\n",
    "        \n",
    "    i += 1\n",
    "print(\"DONE\")\n",
    "#the individual files for the months loaded is saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding subsequent months data to existing file\n",
    "The process is done in batching due to memory constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming combined_house_dataframes already contains the data for the first 4 months for each house_id\n",
    "save_dir= r'C:\\DATA\\ckw\\spring'\n",
    "\n",
    "# Process each common ID\n",
    "for house_id in combined_house_dataframes:\n",
    "    # Get the existing concatenated dataframe for the current house_id\n",
    "    concatenated_house_data = combined_house_dataframes[house_id]\n",
    "    \n",
    "    # Initialize an empty list to store dataframes for the next 4 months\n",
    "    next_house_data_list = []\n",
    "    print(f'Adding next 4 months for house ID: {house_id}')\n",
    "    i=0\n",
    "    # Iterate over the next 4 monthly dataframes\n",
    "    for key, df in list(dataframes.items())[3:5]:  # Adjust the slice [4:8] for the next 4 months\n",
    "        print(f'Processing dataframe: {key}')\n",
    "        # Filter the current dataframe for the current house ID\n",
    "        house_data = df[df['id'] == house_id].copy()\n",
    "\n",
    "        # Check for duplicate rows within each day's data\n",
    "        house_data = house_data.drop_duplicates(subset=['timestamp', 'id'])\n",
    "        \n",
    "        # Append the filtered dataframe to the list\n",
    "        next_house_data_list.append(house_data)\n",
    "    \n",
    "    # Concatenate all dataframes for the next 4 months\n",
    "    concatenated_next_data = pd.concat(next_house_data_list, ignore_index=True)\n",
    "    \n",
    "    # Concatenate with the existing data for the house_id\n",
    "    updated_house_data = pd.concat([concatenated_house_data, concatenated_next_data], ignore_index=True)\n",
    "    \n",
    "    # Update the dictionary entry for the house_id\n",
    "    combined_house_dataframes[house_id] = updated_house_data\n",
    "\n",
    "# Now combined_house_dataframes contains the data for the first 8 months for each house_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each concatenated DataFrame to a CSV file\n",
    "i=0\n",
    "for house_id, df in combined_house_dataframes.items():\n",
    "    file_name = f'{i}_data.csv'\n",
    "    df1=df.sort_values(by='timestamp')\n",
    "    df1.to_csv(file_name, index=False)\n",
    "    print(f\"Data for house ID: {house_id} saved to {file_name}\")\n",
    "\n",
    "    # Verify the number of rows for each house ID\n",
    "    print(f\"House ID {house_id} has {len(df1)} rows. Expected 35040 rows.\") $\n",
    "    #for the whole year 365 day x 24 hours x 4 in each hour\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filling the missing data with 0 if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for winter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "winter_timestamp_range = pl.datetime_range(datetime(2021,12,1), datetime(2022,2,28,23,45), \"15m\",eager=True)\n",
    "# Function to ensure the dataset has the full range of timestamps\n",
    "def fill_missing_timestamps(df, full_range):\n",
    "    # Convert the timestamp column to datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    \n",
    "    # Create a DataFrame with the full range of timestamps\n",
    "    full_range_df = pd.DataFrame({'timestamp': full_range})\n",
    "    full_range_df = pd.to_datetime(full_range_df['timestamp'], utc=True)\n",
    "    # Merge the original DataFrame with the full range DataFrame\n",
    "    merged_df = pd.merge(full_range_df, df, on='timestamp', how='left')\n",
    "    \n",
    "    # Fill missing values with 0\n",
    "    merged_df['value_kwh'].fillna(0, inplace=True)\n",
    "    \n",
    "    return merged_df\n",
    "if season == \"winter\":\n",
    "    for i,file_path in enumerate(file_paths):\n",
    "        df = pd.read_csv(file_path)\n",
    "        #df = df.tail( df.shape[0] -3)\n",
    "        #df.write_csv(file_path)\n",
    "        #print(f\"no. of rows in {i} =\",len(df))\n",
    "        if len(df)<8640:\n",
    "            # Ensure the dataset has the full range of timestamps and fill missing values\n",
    "            df_filled = fill_missing_timestamps(df, winter_timestamp_range)\n",
    "            # Save the updated dataset back to CSV\n",
    "            df_filled.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilmtk-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
